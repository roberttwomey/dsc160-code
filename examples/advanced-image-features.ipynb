{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSC160 Data Science and the Arts - Twomey - Spring 2020 - [dsc160.roberttwomey.com](http://dsc160.roberttwomey.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Image Features\n",
    "\n",
    "There are a variety of advanced image features we can use to develop more nuanced interpretations of images. This notebook demonstrates basic edge calculation, a Hough line transform, and pixel-wise entropy within the image.\n",
    "\n",
    "This notebook isn't exhaustive: I'd suggest you explore [opencv](https://opencv.org/), [scikit-image](https://scikit-image.org/), and other image processing libraries to discover additional possbilities for an image-based cultural analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Edges Within an Image\n",
    "\n",
    "There are a variety of ways to calculate edges within an image. The resulting edge image can be the basis for contour detection or other advanced processes, or can be used to directly estimate the number of pixels (amount) of high spatial frequency transitions within the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soble Operator for Edge Detection\n",
    "\n",
    "The Sobel operator performs a 2-D spatial gradient measurement on an image and so emphasizes regions of high spatial frequency that correspond to edges. \n",
    "\n",
    "The sobel filter uses two 3 x 3 kernels. One for changes in the horizontal direction, and one for changes in the vertical direction. The two kernels are convolved with the original image to calculate the approximations of the derivatives\n",
    "\n",
    "If we define Gx and Gy as two images that contain the horizontal and vertical derivative approximations respectively, the computations are:\n",
    "\n",
    "${ G }_{ x }=\\left[ \\begin{matrix} 1 & 0 & -1 \\\\ 2 & 0 & -2 \\\\ 1 & 0 & -1 \\end{matrix} \\right] *Image $\n",
    "\n",
    "${ G }_{ y }=\\left[ \\begin{matrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ 1 & 2 & 1 \\end{matrix} \\right] *Image $\n",
    "\n",
    "At each pixel in the image, the gradient approximations given by ${ G }_{ x }$ and ${ G }_{ y }$ are combined to give the gradient magnitude, using $\\sqrt { { { G }_{ x } }^{ 2 }+{ { G }_{ y } }^{ 2 } } $\n",
    "\n",
    "The gradient’s direction is calculated using: $\\Theta =arctan\\left( \\frac { { G }_{ x } }{ { G }_{ y } }  \\right) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate edges within the value channel of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "import scipy.misc\n",
    "from skimage import data, io\n",
    "from skimage.color import rgb2hsv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load a test image and convert it to hsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face = scipy.misc.face()\n",
    "rgb_img = face\n",
    "hsv_img = rgb2hsv(rgb_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the H, S, and V channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue_img = hsv_img[:, :, 0]\n",
    "saturation_img = hsv_img[:,:, 1]\n",
    "value_img = hsv_img[:, :, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the H, S, V images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1, ax2, ax3) = plt.subplots(ncols=4, figsize=(16, 8))\n",
    "\n",
    "ax0.imshow(rgb_img)\n",
    "ax0.set_title(\"RGB image\")\n",
    "ax0.axis('off')\n",
    "ax1.imshow(hue_img, cmap='hsv')\n",
    "ax1.set_title(\"Hue channel (H)\")\n",
    "ax1.axis('off')\n",
    "ax2.imshow(saturation_img)\n",
    "ax2.set_title(\"Saturation channel (S)\")\n",
    "ax2.axis('off')\n",
    "ax3.imshow(value_img)\n",
    "ax3.set_title(\"Value channel (V)\")\n",
    "ax3.axis('off')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the ndimage (multi-dimensional image) sobel filter method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobel_x = ndimage.sobel(value_img, axis=0, mode='constant')\n",
    "sobel_y = ndimage.sobel(value_img, axis=1, mode='constant')\n",
    "edge_image = np.hypot(sobel_x, sobel_y)\n",
    "\n",
    "ax = plt.imshow(edge_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the edges correspond to the areas of sharp contrast/pixel-pixel transition within the value image above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Edges on a painting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painting = io.imread(\"https://images.rkd.nl/rkd/thumb/650x650/985b50b4-1378-78dd-52ce-1eee52771b4f.jpg\")\n",
    "ax = plt.imshow(painting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the image to HSV and extract the value channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv_img = rgb2hsv(painting)\n",
    "value_img = hsv_img[:, :, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.imshow(value_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the x and y component of the sobel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobel_x = ndimage.sobel(value_img, axis=0, mode='constant')\n",
    "sobel_y = ndimage.sobel(value_img, axis=1, mode='constant')\n",
    "edge_image = np.hypot(sobel_x, sobel_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the resulting edge image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.imshow(edge_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: the resulting edge image shows high intensity where hard edges are found within an image, and low intensity in areas of uniform color. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of the edge image (normalized by pixel count) is an approximation of how many edges are found within the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sum of edge image:\", np.sum(edge_image)/ edge_image.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV Implementations of Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV (computer vision) has lots of great advanced image processing techniques: https://opencv.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your working platform, we may need to install opencv. The following pip command will install opencv and oencv-contrib (additional methods) for the current user on datahub. Uncomment (remove the `#`) and then run the cell. (FYI the `!` is jupyter magic to run a bash command, as if you opened a terminal and executed the following text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python-headless --user\n",
    "#!pip install opencv-contrib-python-headless --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules and do some setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "from skimage.color import rgb2hsv\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using OpenCV to find features in two paintings by Piet Mondrian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load two images of paintings from the artist Piet Mondrian, a prototypical example of an artist who over the course of his career progressed from a realistic/representational style to pure geometric abstraction. See his complete works here: [http://pietmondrian.rkdmonographs.nl/](http://pietmondrian.rkdmonographs.nl/).\n",
    "\n",
    "We will load in one abstract painting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = io.imread(\"https://images.rkd.nl/rkd/thumb/650x650/985b50b4-1378-78dd-52ce-1eee52771b4f.jpg\")\n",
    "ax = io.imshow(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and one landscape image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landscape = io.imread('https://images.rkd.nl/rkd/thumb/650x650/bcb9558d-08a1-a57f-b5fc-ec562c446838.jpg')\n",
    "ax = io.imshow(landscape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate edge images for each painting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab value channel\n",
    "hsv_img = rgb2hsv(landscape)\n",
    "value_img = hsv_img[:, :, 2]\n",
    "\n",
    "# calculate sobel in x and y directions\n",
    "sobel_x = ndimage.sobel(value_img, axis=1, mode='constant')\n",
    "sobel_y = ndimage.sobel(value_img, axis=0, mode='constant')\n",
    "landscape_edge_image = np.hypot(sobel_x, sobel_y)\n",
    "\n",
    "# display figure\n",
    "fig, (ax0, ax1, ax2, ax3) = plt.subplots(ncols=4, figsize=(16, 4))\n",
    "\n",
    "ax0.imshow(landscape)\n",
    "ax0.set_title(\"Painting\")\n",
    "ax0.axis('off')\n",
    "ax1.imshow(landscape_edge_image, cmap='gray')\n",
    "ax1.set_title(\"Edge (Magnitude of X-Y Sobel)\")\n",
    "ax1.axis('off')\n",
    "ax2.imshow(sobel_x, cmap='gray')\n",
    "ax2.set_title(\"X Sobel\")\n",
    "ax2.axis('off')\n",
    "ax3.imshow(sobel_y, cmap='gray')\n",
    "ax3.set_title(\"Y Sobel\")\n",
    "ax3.axis('off')\n",
    "fig.tight_layout()\n",
    "\n",
    "# print stats\n",
    "print(\"mean of edge_image (edginess):\", landscape_edge_image.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab value channel\n",
    "hsv_img = rgb2hsv(abstract)\n",
    "value_img = hsv_img[:, :, 2]\n",
    "\n",
    "# calculate sobel in x and y directions\n",
    "sobel_x = ndimage.sobel(value_img, axis=1, mode='constant')\n",
    "sobel_y = ndimage.sobel(value_img, axis=0, mode='constant')\n",
    "abstract_edge_image = np.hypot(sobel_x, sobel_y)\n",
    "\n",
    "# display figure\n",
    "fig, (ax0, ax1, ax2, ax3) = plt.subplots(ncols=4, figsize=(16, 4))\n",
    "\n",
    "ax0.imshow(abstract)\n",
    "ax0.set_title(\"Painting\")\n",
    "ax0.axis('off')\n",
    "ax1.imshow(sobel_x, cmap='gray')\n",
    "ax1.set_title(\"X Sobel\")\n",
    "ax1.axis('off')\n",
    "ax2.imshow(sobel_y, cmap='gray')\n",
    "ax2.set_title(\"Y Sobel\")\n",
    "ax2.axis('off')\n",
    "ax3.imshow(abstract_edge_image, cmap='gray')\n",
    "ax3.set_title(\"Edge Image (Magnitude of X,Y Sobel)\")\n",
    "ax3.axis('off')\n",
    "fig.tight_layout()\n",
    "\n",
    "# print stats\n",
    "print(\"mean of edge_image (edginess):\", abstract_edge_image.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Hough Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hough transform is a feature extraction method for detecting simple shapes such as circles, lines etc in an image.\n",
    "\n",
    "It involves:\n",
    "- representing a shape in mathematical form. For example, a line can be represented by two parameters (slope, intercept) and a circle has three parameters — the coordinates of the center and the radius (x, y, r).\n",
    "- building a histogram using the parameters used to represent the shape\n",
    "- thresholding the histogram to output presence of shapes\n",
    "\n",
    "For example, consider hough lines.\n",
    "\n",
    "We know the polar form of a line is represented as:\n",
    "$\\rho =x\\cos { \\theta  } +y\\sin { \\theta  } $\n",
    "\n",
    "Here $\\rho$ represents the perpendicular distance of the line from the origin in pixels, and $\\theta$ is the angle measured in radians.\n",
    "When we say that a line in 2D space is parameterized by $\\rho$ and $\\theta$, it means that if we any pick a ($\\rho$, $\\theta$), it corresponds to a line.\n",
    "\n",
    "We build an accumulator matrix, a histogram matrix with each cell denoting number of pixels parametrized by same $\\rho$ and $\\theta$\n",
    "\n",
    "We can simply select the bins in the accumulator above a certain threshold to find the lines in the image. If the threshold is higher, you will find fewer strong lines, and if it is lower, you will find a large number of lines including some weak ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from skimage to opencv image format\n",
    "from skimage import exposure\n",
    "temp = exposure.rescale_intensity(abstract_edge_image, out_range=(-1.0, 1.0))\n",
    "edges = skimage.img_as_ubyte(np.clip(temp, -1, 1))\n",
    "\n",
    "# calculate hough lines\n",
    "lines = cv2.HoughLines(edges,1,np.pi/180,100)\n",
    "result = np.copy(abstract)\n",
    "for x in range(0, len(lines)):    \n",
    "    for rho, theta in lines[x]:\n",
    "        a = np.cos(theta)\n",
    "        b = np.sin(theta)\n",
    "        x0 = a*rho\n",
    "        y0 = b*rho\n",
    "        x1 = int(x0 + 1000*(-b))\n",
    "        y1 = int(y0 + 1000*(a))\n",
    "        x2 = int(x0 - 1000*(-b))\n",
    "        y2 = int(y0 - 1000*(a))\n",
    "\n",
    "        cv2.line(result,(x1,y1),(x2,y2),(255,255,0),2)\n",
    "\n",
    "# show results\n",
    "ax = plt.imshow(result[:,:,::])\n",
    "# cv2.imwrite('houghlines3.jpg',img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many horizontal/vertical lines did we find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension: try this again with the landscape image above, and see if we detect any horizontal or vertical lines.\n",
    "\n",
    "Further reading on the Hough Transform on [opencvy python tutorials](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_houghlines/py_houghlines.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Hough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic Hough Transform is an optimization of Hough Transform. It doesn’t take all the input points into consideration, instead take only a random subset of points and that is sufficient for shape detection.\n",
    "\n",
    "One of the easiest probabilstic method is to choose m input points from the set M input points. The complexity of the voting stage reduces from $O(M.{ N }_{ \\Theta  })$ to $O(m.{ N }_{ \\Theta  })$. This works because a random subset of M will fairly represent the edge points and the surrounding noise and distrotion.\n",
    "\n",
    "Smaller value of m will result in fast computation but lower accuracy. So the value of m should be appropriately chosen with respect to M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = exposure.rescale_intensity(abstract_edge_image, out_range=(-1.0, 1.0))\n",
    "\n",
    "edges = skimage.img_as_ubyte(np.clip(temp, -1, 1))\n",
    "\n",
    "# Probabilistic Hough Transform\n",
    "minLineLength = 400\n",
    "maxLineGap = 10\n",
    "\n",
    "lines = cv2.HoughLinesP(edges,1,np.pi/180,100,minLineLength,maxLineGap)\n",
    "result = abstract.copy()\n",
    "\n",
    "for x in range(0, len(lines)):    \n",
    "    for x1,y1,x2,y2 in lines[x]:\n",
    "        cv2.line(result,(x1,y1),(x2,y2),(0,255,255),5)\n",
    "\n",
    "# cv2.imwrite('houghlines5.jpg',edges)\n",
    "ax = plt.imshow(result[:,:,::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy H of an image is defined as\n",
    "\n",
    "$H=-\\sum _{ k=0 }^{ M-1 }{ { p }_{ k }\\log _{ 2 }{ ({ p }_{ k }) }  } $\n",
    "\n",
    "Entropy can be considered as the spread of states. A low entropy system occupies a small number of such states, while a high entropy system occupies a large number of states.\n",
    "\n",
    "For example, in an 8-bit pixel there are 256 such states. If all such states are equally occupied, as they are in the case of an image which has been perfectly histogram equalized, the spread of states is a maximum, as is the entropy of the image. On the other hand, if the image has been thresholded, so that only two states are occupied, the entropy is low. If all of the pixels have the same value, the entropy of the image is zero.\n",
    "\n",
    "The example below iterates across all positions in the image, calculating the entropy at each point based on the surrounding values within a selected region (in this case a disk of radius 10). In the output image (entropy image) each pixel corresponds to the calculated entropy score for the surrounding values in the original input image.\n",
    "\n",
    "More information on image entropy: https://scikit-image.org/docs/dev/auto_examples/filters/plot_entropy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters.rank import entropy\n",
    "from skimage.morphology import disk\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy calculation using builtin function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = landscape\n",
    "gray_img = rgb2gray(img)\n",
    "entr_img = entropy(gray_img, disk(10))\n",
    "\n",
    "ax = io.imshow(entr_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: `disc(10)` above is the structuring element, a disc of radius 10. Entropy is calculated for each pixel based on the values within a circle of radius 10 around that point. There are other choices of shape and obviously radius/size, which will affect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = io.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = abstract\n",
    "gray_img = rgb2gray(img)\n",
    "entr_img = entropy(gray_img, disk(10))\n",
    "\n",
    "ax = io.imshow(entr_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = io.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "More information on Dual Gradient image energy (in the context of Seam Carving): https://www.cs.princeton.edu/courses/archive/spring13/cos226/assignments/seamCarving.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcDGenergy(img):\n",
    "    # from from https://stackoverflow.com/a/48974892\n",
    "\n",
    "    #convert from uint8 to int64 to prevent overflow problems\n",
    "    arr = np.array(img, dtype = int)\n",
    "\n",
    "    #calculate squared difference ((x-1, y) - (x+1, y))^2 for each R, G and B pixel\n",
    "    deltaX2 = np.square(np.roll(arr, -1, axis = 0) - np.roll(arr, 1, axis = 0))\n",
    "\n",
    "    #same for y axis\n",
    "    deltaY2 = np.square(np.roll(arr, -1, axis = 1) - np.roll(arr, 1, axis = 1))\n",
    "\n",
    "    #add R, G and B values for each pixel, then add x- and y-shifted values\n",
    "    dualEnergy = np.sum(deltaX2, axis = 2) + np.sum(deltaY2, axis = 2)\n",
    "    return dualEnergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgEnergy = calcDGenergy(abstract)\n",
    "plt.imshow(dgEnergy)\n",
    "plt.show()\n",
    "\n",
    "print(\"min: \", dgEnergy.min())\n",
    "print(\"max: \", dgEnergy.max())\n",
    "print(\"mean: \", dgEnergy.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgEnergy = calcDGenergy(landscape)\n",
    "plt.imshow(dgEnergy)\n",
    "plt.show()\n",
    "\n",
    "print(\"min: \", dgEnergy.min())\n",
    "print(\"max: \", dgEnergy.max())\n",
    "print(\"mean: \", dgEnergy.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Directions\n",
    "\n",
    "As mentioned above, there are many further techniques that can be used to extract features from and study/describe paintings.\n",
    "\n",
    "If you have any experience with this, share your ideas with the class on Piazza!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
