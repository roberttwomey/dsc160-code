{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSC160 Data Science and the Arts - Twomey - Spring 2020 - [dsc160.roberttwomey.com](http://dsc160.roberttwomey.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning the GPT-2 Model \n",
    "\n",
    "This notebook explores the GPT-2 model released by OpenAI on February 14th, 2019. This blog post 'Better Language Models and Their Implications' https://openai.com/blog/better-language-models/, discusses the project.\n",
    "\n",
    "Due to concerns over the effectiveness of their model (this reasoning bears questioning), they have only released a smaller version (117M parameters vs 1.5B parameters) of their larger model. See their reasoning [here](https://openai.com/blog/better-language-models/#releasestrategy). They also did not release any of the training software they used to produce the full GPT-2 model.\n",
    "\n",
    "Data: The GPT-2 model was trained on a corpus of 8 million webpages (40GB of text) scraped from outgoing reddit links with Karma > 3. This if a form of human/collaborative filtering, and supposed indicator of \"quality\" of the outgoing links. See their data curation approach [here](https://openai.com/blog/better-language-models/#fn1).  \n",
    "\n",
    "Paper: [Language Models are Unsupervised Task Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "Code: https://github.com/openai/gpt-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning vs. Retraining\n",
    "\n",
    "Training a 1.5B parameter model requires an enormous corpus of text (8M webpages yielding 40GB of plain text). Even with a reduced 117M parameter GPT-2, training this architecture on a small dataset (where data is smaller than the number of parameters) reduces the networks ability to generalize and likely results in overfitting. Fine-tuning a network means taking that high-parameter, pretrained network, and continuing training (via backpropagation) with our new, smaller dataset. The caveat here is that the new dataset needs to resemble/be similar in kind to the original training data.\n",
    "\n",
    "Even if we wanted to train GPT-2 architecture from scratch, OpenAI has not provided access to the training data or training code.\n",
    "\n",
    "The example below wraps up the gpt-2 fine-tuning process in a very simple interface, installed via pip. Written by Max Woolf ([@minimaxir](https://minimaxir.com/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gpt-2-simple --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "\n",
    "model_name = \"124M\"\n",
    "gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/\n",
    "\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.finetune(sess,\n",
    "              'script.txt',\n",
    "              model_name=model_name,\n",
    "              steps=300)#1000)   # steps is max number of training steps\n",
    "\n",
    "gpt2.generate(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activities\n",
    "\n",
    "- Explore different finetuning texts (of your choice) and their results.\n",
    "  - Observe how the size of the text (num characters) relate to the number of parameters of the GPT-2 model you are working with.\n",
    "  - Observe the loss \n",
    "  - Look for over-fitting (loss at or near 0)\n",
    "- Try the other sizes of published models, how do they change the complexity/intelligibility of generated text. gpt_2_simple wraps the \"small\" 124M and \"medium\" 355M models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- Example is from [https://github.com/minimaxir/gpt-2-simple](https://github.com/minimaxir/gpt-2-simple). \n",
    "  - See his other projects here: https://minimaxir.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
